The first time you run Airflow, it will create a file called airflow.cfg in your $AIRFLOW_HOME directory (~/airflow by default). 
This file contains Airflow’s configuration and you can edit it to change any of the settings. You can find this file and search for web_server_port. Then change it to any unused port, e.g. 8081. This will change the port from 8080 to something like 8081.
You can then run airflow webserver as usual, http://localhost:8081/

-------------------------------------------------------------------------------------------------------------------------------------
If you want to use PostgreSQL database for Airflow, follow the steps:

You need to create a database and a database user that Airflow will use to access this database. In the example below, a database "airflow_db" and user with username "airflow_user" with password "airflow_pass" will be created

CREATE DATABASE airflow_db;
CREATE USER airflow_user WITH PASSWORD 'airflow_pass';
GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;

Then, on airflow.cfg file, the adjustment we will do is to change the executor value to LocalExecutor

executor = LocalExecutor
sql_alchemy_conn = postgresql+psycopg2://<user>:<password>@<host>/<db>

The default executor is SequentialExecutor. In LocalExecutor, task instances get executed as subprocesses. https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-postgresql-database

Then, on Terminal, initialize the database and create the user:

python3.10 -m airflow db init
python3.10 -m airflow users create --username mmuratarat --firstname Murat --lastname Arat --role Admin --email arat.murat@gmail.com

--------------------------------------------------------------------------------------------------------------------------------------
python3 -m airflow webserver --port 8081 will start the server on host 8081 attached to terminal. 
python3 -m airflow scheduler -D will start the scheduler detached.
-------------------------------------------------------------------------------------------------------------------------------------

ps aux | grep airflow will show all airflow processes.

cat ~/airflow/airflow-scheduler.pid will return PID of Airflow Scheduler
cat ~/airflow/airflow-webserver.pid will return PID of Airflow Webserver

Airflow creates files with process IDs of the scheduler and gunicorn server in its home directory (by default ~/airflow/). Running

kill $(cat ~/airflow/airflow-scheduler.pid)

Will terminate the scheduler.

Unfortunately, airflow-webserver.pid contains the PID of the gunicorn server and not the initial Airflow command that started it (which is the parent of the gunicorn process). So, we will first have to find the parent PID of the gunicorn process and then kill the parent process.

kill -9 $(ps -o ppid= -p $(cat ~/airflow/airflow-webserver-monitor.pid))

kill $(cat ~/airflow/airflow-webserver-monitor.pid)

------------------------------------------------------------------------------------------------------------------------------------------

## Some Commands

"python3.10 -m airflow db check" will check the status of your database (eg. connected, not connected, etc).

"python3.10 -m airflow users list" will list down all users and their roles.

"python3.10 -m airflow tasks list <DAG_ID>" will list down all tasks related to a given DAG.

"python3.10 -m airflow dags list" will list our a list of DAGs that you currently have running.

"python3.10 -m airflow dags delete <DAG_ID>" will delete all the data in DB related to the task.

"python3.10 -m airflow dags show <DAG_ID>" will show the structure and dependencies of a DAG.

-------------------------------------------------------------------------------------------------------------------------------------

If you get "Postgres + Airflow db: permission denied for schema public" error, "ALTER DATABASE airflow_db OWNER TO airflow_user;" sets the owner of the database from the standard/admin user (e.g. postgres) to airflow_user. Now, run "python3.10 -m airflow db init" again, and the error doesn't come up, instead, you get "Initialization done".

-------------------------------------------------------------------------------------------------------------------------------------

Correct path to your DAG folder is set in airflow.cfg file (it's located at "~/airflow.cfg")

For DAGBag approach, https://xnuinside.medium.com/how-to-load-use-several-dag-folders-airflow-dagbags-b93e4ef4663c

-------------------------------------------------------------------------------------------------------------------------------------
Traditionally, operator relationships are set with the set_upstream() and set_downstream() methods. In Airflow 1.8, this can be done with the Python bitshift operators >> and <<. The following four statements are all functionally equivalent:

op1 >> op2
op1.set_downstream(op2)

op2 << op1
op2.set_upstream(op1)

When using the bitshift to compose operators, the relationship is set in the direction that the bitshift operator points. For example, op1 >> op2 means that op1 runs first and op2 runs second. Multiple operators can be composed – keep in mind the chain is executed left-to-right and the rightmost object is always returned. For example:

op1 >> op2 >> op3 << op4

is equivalent to:

op1.set_downstream(op2)
op2.set_downstream(op3)
op3.set_upstream(op4)


