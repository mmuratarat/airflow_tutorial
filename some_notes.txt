The first time you run Airflow, it will create a file called airflow.cfg in your $AIRFLOW_HOME directory (~/airflow by default). 
This file contains Airflow’s configuration and you can edit it to change any of the settings. You can find this file and search for web_server_port. Then change it to any unused port, e.g. 8081. This will change the port from 8080 to something like 8081.
You can then run airflow webserver as usual, http://localhost:8081/

-------------------------------------------------------------------------------------------------------------------------------------
If you want to use PostgreSQL database for Airflow, follow the steps:

You need to create a database and a database user that Airflow will use to access this database. In the example below, a database "airflow_db" and user with username "airflow_user" with password "airflow_pass" will be created

CREATE DATABASE airflow_db;
CREATE USER airflow_user WITH PASSWORD 'airflow_pass';
GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;

Then, on airflow.cfg file, the adjustment we will do is to change the executor value to LocalExecutor

executor = LocalExecutor
sql_alchemy_conn = postgresql+psycopg2://<user>:<password>@<host>/<db>

The default executor is SequentialExecutor. In LocalExecutor, task instances get executed as subprocesses. https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-postgresql-database

Then, on Terminal, initialize the database and create the user:

python3.10 -m airflow db init
python3.10 -m airflow users create --username mmuratarat --firstname Murat --lastname Arat --role Admin --email arat.murat@gmail.com

--------------------------------------------------------------------------------------------------------------------------------------
python3 -m airflow webserver --port 8081 will start the server on host 8081 attached to terminal. 
python3 -m airflow scheduler -D will start the scheduler detached.
-------------------------------------------------------------------------------------------------------------------------------------

ps aux | grep airflow will show all airflow processes.

cat ~/airflow/airflow-scheduler.pid will return PID of Airflow Scheduler
cat ~/airflow/airflow-webserver.pid will return PID of Airflow Webserver

Airflow creates files with process IDs of the scheduler and gunicorn server in its home directory (by default ~/airflow/). Running

kill $(cat ~/airflow/airflow-scheduler.pid)

Will terminate the scheduler.

Unfortunately, airflow-webserver.pid contains the PID of the gunicorn server and not the initial Airflow command that started it (which is the parent of the gunicorn process). So, we will first have to find the parent PID of the gunicorn process and then kill the parent process.

kill -9 $(ps -o ppid= -p $(cat ~/airflow/airflow-webserver-monitor.pid))

kill $(cat ~/airflow/airflow-webserver-monitor.pid)

------------------------------------------------------------------------------------------------------------------------------------------

## Some Commands

"python3.10 -m airflow db check" will check the status of your database (eg. connected, not connected, etc).

"python3.10 -m airflow users list" will list down all users and their roles.

"python3.10 -m airflow tasks list <DAG_ID>" will list down all tasks related to a given DAG.

"python3.10 -m airflow dags list" will list our a list of DAGs that you currently have running.

"python3.10 -m airflow dags delete <DAG_ID>" will delete all the data in DB related to the task.

"python3.10 -m airflow dags show <DAG_ID>" will show the structure and dependencies of a DAG.

-------------------------------------------------------------------------------------------------------------------------------------

If you get "Postgres + Airflow db: permission denied for schema public" error, "ALTER DATABASE airflow_db OWNER TO airflow_user;" sets the owner of the database from the standard/admin user (e.g. postgres) to airflow_user. Now, run "python3.10 -m airflow db init" again, and the error doesn't come up, instead, you get "Initialization done".

-------------------------------------------------------------------------------------------------------------------------------------

Correct path to your DAG folder is set in airflow.cfg file (it's located at "~/airflow.cfg")

For DAGBag approach, https://xnuinside.medium.com/how-to-load-use-several-dag-folders-airflow-dagbags-b93e4ef4663c

-------------------------------------------------------------------------------------------------------------------------------------

Traditionally, operator relationships are set with the set_upstream() and set_downstream() methods. In Airflow 1.8, this can be done with the Python bitshift operators >> and <<. The following four statements are all functionally equivalent:

op1 >> op2
op1.set_downstream(op2)

op2 << op1
op2.set_upstream(op1)

When using the bitshift to compose operators, the relationship is set in the direction that the bitshift operator points. For example, op1 >> op2 means that op1 runs first and op2 runs second. Multiple operators can be composed – keep in mind the chain is executed left-to-right and the rightmost object is always returned. For example:

op1 >> op2 >> op3 << op4

is equivalent to:

op1.set_downstream(op2)
op2.set_downstream(op3)
op3.set_upstream(op4)

-------------------------------------------------------------------------------------------------------------------------------------

EXECUTORS IN AIRFLOW

Majority types of the Airflow executor instrument tasks to run in a distributed manner, those tasks are running on either multi-processing or multiple worker nodes. An Airflow production environment usually has hundreds of DAGs, which include thousands of tasks to run. With the capability to run various tasks in parallel at such a large scale, the Airflow executor shines on the intensive workload. One of the reasons you have various types of executors is you have the option to choose based on your requirement and infrastructure. Airflow provides executors “toolkit,” different kinds of executors give the flexibility to have Airflow integrate smoothly with your environment.

Sequential Executor: It is a lightweight local executor, which is available by default in airflow. It runs only one task instance at a time in a linear fashion with no parallelism functionality (A → B → C). It does identify a single point of failure, making it helpful for debugging. But it is not production-ready.  Therefore, the Sequential executor is not recommended for any use cases that require more than a single task execution at a time. Sequential Executor can run with SQLite since SQLite does not support multiple connections. It is prone to single-point failure, and we can utilize it for debugging purposes.

Local Executor: Unlike the sequential executor, the local executor can run multiple task instances. Generally, we use MySQL or PostgreSQL databases with the local executor since they allow multiple connections, which helps us achieve parallelism.  Running Apache Airflow on a Local executor exemplifies single-node architecture. The Local executor completes tasks in parallel that run on a single machine (think: your laptop, an EC2 instance, etc.) - the same machine that houses the Scheduler and all code necessary to execute. A single LocalWorker picks up and runs jobs as they’re scheduled and is fully responsible for all task execution. The Local executor is ideal for testing.

Celery Executor: Celery is a task queue, which helps in distributing tasks across multiple celery workers. The Celery Executor distributes the workload from the main application onto multiple celery workers with the help of a message broker such as RabbitMQ or Redis. The executor publishes a request to execute a task in the queue, and one of several worker nodes picks up the request and runs as per the directions provided. MySQL or PostgreSQL database systems are required to set up the Celery Executor. It is a remote executor, which we use for horizontal scaling where workers get distributed across multiple machines in a pipeline. It also allows for real-time processing and task scheduling. It is fault-tolerant, unlike the local executors.

In Celery Executor, Two databases are also available:
1. QueueBroker
2. ResultBackend

Therefore you need to define these in configuration file of Airflow:

executor = CeleryExecutor
dags_folder = /path/to/your/dags_folder
sql_alchemy_conn = postgresql+psycopg2://<user>:<password>@<host>/<db>

broker_url = redis://redis-hostname:6379/0
result_backend = postgresql+psycopg2://<user>:<password>@<host>/<db>

Kubernetes Executor: The Kubernetes Executor uses the Kubernetes API for resource optimization. It runs as a fixed-single Pod in the scheduler that only requires access to the Kubernetes API. A Pod is the smallest deployable object in Kubernetes. When a DAG submits a task, the Kubernetes Executor requests a worker pod. The worker pod is called from the Kubernetes API, which runs the job, reports the result, and terminates once the job gets finished.MySQL or PostgreSQL database systems are required to set up the Kubernetes Executor. It does not require additional components such as a message broker like Celery but a Kubernetes environment. One of the advantages of Kubernetes Executors over the other executors is that Pods only run when tasks are required to be executed, which helps to save resources when there are no jobs to run. In other executors, the workers are statically configured and are running all the time, regardless of workloads. It can automatically scale up to meet the workload requirements, scale down to zero (if no DAGs or tasks are running), and is fault-tolerant.

CeleryKubernetes Executor: It permits users to run Celery Executor and a Kubernetes Executor simultaneously.

Dask Executor: Dask is a parallel computing library in python whose architecture revolves around a sophisticated distributed task scheduler. Dask Executor allows you to run Airflow tasks in a Dask Distributed cluster. It acts as an alternative to the Celery Executor for horizontal scaling, which you implement with good understanding. 
