The first time you run Airflow, it will create a file called airflow.cfg in your $AIRFLOW_HOME directory (~/airflow by default). 
This file contains Airflow’s configuration and you can edit it to change any of the settings. You can find this file and search for web_server_port. Then change it to any unused port, e.g. 8081. This will change the port from 8080 to something like 8081.
You can then run airflow webserver as usual, http://localhost:8081/

-------------------------------------------------------------------------------------------------------------------------------------
If you want to use PostgreSQL database for Airflow, follow the steps:

You need to create a database and a database user that Airflow will use to access this database. In the example below, a database "airflow_db" and user with username "airflow_user" with password "airflow_pass" will be created

CREATE DATABASE airflow_db;
CREATE USER airflow_user WITH PASSWORD 'airflow_pass';
GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;

Then, on airflow.cfg file, the adjustment we will do is to change the executor value to LocalExecutor

executor = LocalExecutor
sql_alchemy_conn = postgresql+psycopg2://<user>:<password>@<host>/<db>

The default executor is SequentialExecutor. In LocalExecutor, task instances get executed as subprocesses. https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-postgresql-database

Then, on Terminal, initialize the database and create the user:

python3.10 -m airflow db init
python3.10 -m airflow users create --username mmuratarat --firstname Murat --lastname Arat --role Admin --email arat.murat@gmail.com

--------------------------------------------------------------------------------------------------------------------------------------
python3 -m airflow webserver --port 8081 will start the server on host 8081 attached to terminal. 
python3 -m airflow scheduler -D will start the scheduler detached.
-------------------------------------------------------------------------------------------------------------------------------------

ps aux | grep airflow will show all airflow processes.

cat ~/airflow/airflow-scheduler.pid will return PID of Airflow Scheduler
cat ~/airflow/airflow-webserver.pid will return PID of Airflow Webserver

Airflow creates files with process IDs of the scheduler and gunicorn server in its home directory (by default ~/airflow/). Running

kill $(cat ~/airflow/airflow-scheduler.pid)

Will terminate the scheduler.

Unfortunately, airflow-webserver.pid contains the PID of the gunicorn server and not the initial Airflow command that started it (which is the parent of the gunicorn process). So, we will first have to find the parent PID of the gunicorn process and then kill the parent process.

kill -9 $(ps -o ppid= -p $(cat ~/airflow/airflow-webserver-monitor.pid))

kill $(cat ~/airflow/airflow-webserver-monitor.pid)

------------------------------------------------------------------------------------------------------------------------------------------

## Some Commands

"python3.10 -m airflow db check" will check the status of your database (eg. connected, not connected, etc).

"python3.10 -m airflow users list" will list down all users and their roles.

"python3.10 -m airflow tasks list <DAG_ID>" will list down all tasks related to a given DAG.

"python3.10 -m airflow dags list" will list our a list of DAGs that you currently have running.

"python3.10 -m airflow dags delete <DAG_ID>" will delete all the data in DB related to the task.

"python3.10 -m airflow dags show <DAG_ID>" will show the structure and dependencies of a DAG.

-------------------------------------------------------------------------------------------------------------------------------------

If you get "Postgres + Airflow db: permission denied for schema public" error, "ALTER DATABASE airflow_db OWNER TO airflow_user;" sets the owner of the database from the standard/admin user (e.g. postgres) to airflow_user. Now, run "python3.10 -m airflow db init" again, and the error doesn't come up, instead, you get "Initialization done".

-------------------------------------------------------------------------------------------------------------------------------------

Correct path to your DAG folder is set in airflow.cfg file (it's located at "~/airflow.cfg")

For DAGBag approach, https://xnuinside.medium.com/how-to-load-use-several-dag-folders-airflow-dagbags-b93e4ef4663c

-------------------------------------------------------------------------------------------------------------------------------------

Traditionally, operator relationships are set with the set_upstream() and set_downstream() methods. In Airflow 1.8, this can be done with the Python bitshift operators >> and <<. The following four statements are all functionally equivalent:

op1 >> op2
op1.set_downstream(op2)

op2 << op1
op2.set_upstream(op1)

When using the bitshift to compose operators, the relationship is set in the direction that the bitshift operator points. For example, op1 >> op2 means that op1 runs first and op2 runs second. Multiple operators can be composed – keep in mind the chain is executed left-to-right and the rightmost object is always returned. For example:

op1 >> op2 >> op3 << op4

is equivalent to:

op1.set_downstream(op2)
op2.set_downstream(op3)
op3.set_upstream(op4)

-------------------------------------------------------------------------------------------------------------------------------------

EXECUTORS IN AIRFLOW

Majority types of the Airflow executor instrument tasks to run in a distributed manner, those tasks are running on either multi-processing or multiple worker nodes. An Airflow production environment usually has hundreds of DAGs, which include thousands of tasks to run. With the capability to run various tasks in parallel at such a large scale, the Airflow executor shines on the intensive workload. One of the reasons you have various types of executors is you have the option to choose based on your requirement and infrastructure. Airflow provides executors “toolkit,” different kinds of executors give the flexibility to have Airflow integrate smoothly with your environment.

Sequential Executor: It is a lightweight local executor, which is available by default in airflow. It runs only one task instance at a time in a linear fashion with no parallelism functionality (A → B → C). It does identify a single point of failure, making it helpful for debugging. But it is not production-ready.  Therefore, the Sequential executor is not recommended for any use cases that require more than a single task execution at a time. Sequential Executor can run with SQLite since SQLite does not support multiple connections. It is prone to single-point failure, and we can utilize it for debugging purposes.

Local Executor: Unlike the sequential executor, the local executor can run multiple task instances. Generally, we use MySQL or PostgreSQL databases with the local executor since they allow multiple connections, which helps us achieve parallelism.  Running Apache Airflow on a Local executor exemplifies single-node architecture. The Local executor completes tasks in parallel that run on a single machine (think: your laptop, an EC2 instance, etc.) - the same machine that houses the Scheduler and all code necessary to execute. A single LocalWorker picks up and runs jobs as they’re scheduled and is fully responsible for all task execution. The Local executor is ideal for testing.

Celery Executor: Celery is a task queue, which helps in distributing tasks across multiple celery workers. The Celery Executor distributes the workload from the main application onto multiple celery workers with the help of a message broker such as RabbitMQ or Redis. The executor publishes a request to execute a task in the queue, and one of several worker nodes picks up the request and runs as per the directions provided. MySQL or PostgreSQL database systems are required to set up the Celery Executor. It is a remote executor, which we use for horizontal scaling where workers get distributed across multiple machines in a pipeline. It also allows for real-time processing and task scheduling. It is fault-tolerant, unlike the local executors.

In Celery Executor, Two databases are also available:
1. QueueBroker
2. ResultBackend

Therefore you need to define these in configuration file of Airflow:

executor = CeleryExecutor
dags_folder = /path/to/your/dags_folder
sql_alchemy_conn = postgresql+psycopg2://<user>:<password>@<host>/<db>

broker_url = redis://redis-hostname:6379/0
result_backend = db+postgresql+psycopg2://<user>:<password>@<host>/<db>

Remeber to add the "db+" prefix to the database connection string:

Kubernetes Executor: The Kubernetes Executor uses the Kubernetes API for resource optimization. It runs as a fixed-single Pod in the scheduler that only requires access to the Kubernetes API. A Pod is the smallest deployable object in Kubernetes. When a DAG submits a task, the Kubernetes Executor requests a worker pod. The worker pod is called from the Kubernetes API, which runs the job, reports the result, and terminates once the job gets finished.MySQL or PostgreSQL database systems are required to set up the Kubernetes Executor. It does not require additional components such as a message broker like Celery but a Kubernetes environment. One of the advantages of Kubernetes Executors over the other executors is that Pods only run when tasks are required to be executed, which helps to save resources when there are no jobs to run. In other executors, the workers are statically configured and are running all the time, regardless of workloads. It can automatically scale up to meet the workload requirements, scale down to zero (if no DAGs or tasks are running), and is fault-tolerant.

CeleryKubernetes Executor: It permits users to run Celery Executor and a Kubernetes Executor simultaneously.

Dask Executor: Dask is a parallel computing library in python whose architecture revolves around a sophisticated distributed task scheduler. Dask Executor allows you to run Airflow tasks in a Dask Distributed cluster. It acts as an alternative to the Celery Executor for horizontal scaling, which you implement with good understanding. 

-------------------------------------------------------------------------------------------------------------------------------------

If you using airflow with Docker, You can enter the worker container so that you can run airflow commands 

docker exec -it <container-id> bash

You can find <container-id> for the Airflow worker service by running "docker ps".

After you are in bash shell of the container, you can list all the dags with the command:

airflow dags list

-------------------------------------------------------------------------------------------------------------------------------------

OPERATORS IN AIRFLOW

Airflow provides operators for many common tasks, including:

- BashOperator - executes a bash command
- PythonOperator - calls an arbitrary Python function
- EmailOperator - sends an email
- SimpleHttpOperator - sends an HTTP request
- MySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, JdbcOperator, etc. - executes a SQL command
- Sensor - waits for a certain time, file, database row, S3 key, etc…

In addition to these basic building blocks, there are many more specific operators: DockerOperator, HiveOperator, S3FileTransformOperator(, PrestoToMySqlTransfer, SlackAPIOperator… you get the idea!

Operators are only loaded by Airflow if they are assigned to a DAG.

-------------------------------------------------------------------------------------------------------------------------------------

XCOMS

XComs let tasks exchange messages, allowing more nuanced forms of control and shared state. The name is an abbreviation of “cross-communication”. XComs are principally defined by a key, value, and timestamp.

XComs can be “pushed” (sent) or “pulled” (received). 

-------------------------------------------------------------------------------------------------------------------------------------

AIRFLOW TRIGGER RULES

By default, your tasks get executed once all the parent tasks succeed. this behaviour is what you expect in general. But what if you want something more complex? What if you would like to execute a task as soon as one of its parents succeeds? Or maybe you would like to execute a different set of tasks if a task fails? Or act differently according to if a task succeeds, fails or event gets skipped? 




all_success
This one is pretty straightforward, and you’ve already seen it, your task gets triggered when all upstream tasks (parents) have succeeded.

One caveat though, if one of the parents gets skipped, then the task gets skipped as well




all_failed
Pretty clear, your task gets triggered if all of its parent tasks have failed.

Like with all_success, if one of the parents gets skipped, then the task gets skipped as well




one_failed
As soon as one of the upstream tasks fails, your task gets triggered.

Can be useful if you have some long running tasks and want to do something as soon as one fails.




one_success
Like with one_failed, but the opposite. As soon as one of the upstream tasks succeeds, your task gets triggered.




none_failed
Your task gets triggered if all upstream tasks have succeeded or been skipped.

Only useful if you want to handle the skipped status.




none_failed_min_one_success
Before known as “none_failed_or_skipped” (before Airflow 2.2), with this trigger rule, your task gets triggered if all upstream tasks haven’t failed and at least one has succeeded.




none_skipped
With this simple trigger rule, your task gets triggered if no upstream tasks are skipped. If they are all in success or failed.

-------------------------------------------------------------------------------------------------------------------------------------

